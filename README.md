# pre-training_cook

## Uncategorized
- [JetMoe](https://github.com/myshell-ai/JetMoE)
- Reasoning (self-supervise): [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/pdf/2403.09629.pdf)

## Scaling Law
- [MiniCPM](https://github.com/OpenBMB/MiniCPM/tree/main), [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395)
- [Compression Represents Intelligence Linearly](https://arxiv.org/pdf/2404.09937.pdf)

## Data
- [Advancing LLM Reasoning Generalists with Preference Trees](https://arxiv.org/pdf/2404.02078.pdf): sft, preference tuning 
