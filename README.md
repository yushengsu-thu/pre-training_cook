# pre-training_cook (Reading list)


## Other
- [Let's Think Dot by Dot: Hidden Computation in Transformer Language Models](https://arxiv.org/abs/2404.15758)
- [A Survey on Self-Evolution of Large Language Models](https://arxiv.org/pdf/2404.14387)
- [Microsfot phi]()
- [Predicting Emergent Abilities with Infinite Resolution Evaluation](https://arxiv.org/abs/2310.03262) v

## KAN
- [KAN: Kolmogorovâ€“Arnold Networks](https://arxiv.org/abs/2404.19756) v

## Uncategorized
- [JetMoe](https://github.com/myshell-ai/JetMoE)

## Mup 
- [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)
- [Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522)

## Scaling Law
- [MiniCPM](https://github.com/OpenBMB/MiniCPM/tree/main), [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395) v
- [Compression Represents Intelligence Linearly](https://arxiv.org/pdf/2404.09937.pdf)

## RLHF
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)
- [Iterative Reasoning Preference Optimization](https://arxiv.org/abs/2404.19733)

## Data
- [Advancing LLM Reasoning Generalists with Preference Trees](https://arxiv.org/pdf/2404.02078.pdf): v sft, preference tuning 

## Reasoning: weaker-to-stronger, sel-tuning, self-learning
- [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/pdf/2403.09629) v
- [STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)
